Epochs = 3000 | Nodes Dim = 512 | Learning Rate = 0.005 | Batch size = 32 | Activation function = ReLU | Number of Layers = 4 | Division = 0.125  
Tensor Data Shape: torch.Size([24010, 1485]) 
Tensor Time Shape:, torch.Size([24010]) 
train loss: 152.40672302246094, len train_loader: 601 
Epoch: 0 | Train loss: 0.25359 | Test loss: 0.25341 
 train loss: 146.6405792236328, len train_loader: 601 
Epoch: 10 | Train loss: 0.24399 | Test loss: 0.24407 
 train loss: 74.64466094970703, len train_loader: 601 
Epoch: 20 | Train loss: 0.12420 | Test loss: 0.11934 
 train loss: 70.21408081054688, len train_loader: 601 
Epoch: 30 | Train loss: 0.11683 | Test loss: 0.11883 
 train loss: 67.66864013671875, len train_loader: 601 
Epoch: 40 | Train loss: 0.11259 | Test loss: 0.13820 
 train loss: 65.5784912109375, len train_loader: 601 
Epoch: 50 | Train loss: 0.10912 | Test loss: 0.10785 
 train loss: 63.568641662597656, len train_loader: 601 
Epoch: 60 | Train loss: 0.10577 | Test loss: 0.12475 
 train loss: 61.81813049316406, len train_loader: 601 
Epoch: 70 | Train loss: 0.10286 | Test loss: 0.11448 
 train loss: 59.9032096862793, len train_loader: 601 
Epoch: 80 | Train loss: 0.09967 | Test loss: 0.13986 
 train loss: 57.95882797241211, len train_loader: 601 
Epoch: 90 | Train loss: 0.09644 | Test loss: 0.09792 
 train loss: 56.25411605834961, len train_loader: 601 
Epoch: 100 | Train loss: 0.09360 | Test loss: 0.11223 
 train loss: 54.762367248535156, len train_loader: 601 
Epoch: 110 | Train loss: 0.09112 | Test loss: 0.10181 
 train loss: 52.81659698486328, len train_loader: 601 
Epoch: 120 | Train loss: 0.08788 | Test loss: 0.10013 
 train loss: 51.25550842285156, len train_loader: 601 
Epoch: 130 | Train loss: 0.08528 | Test loss: 0.11764 
 train loss: 49.9377555847168, len train_loader: 601 
Epoch: 140 | Train loss: 0.08309 | Test loss: 0.08703 
 train loss: 48.17569351196289, len train_loader: 601 
Epoch: 150 | Train loss: 0.08016 | Test loss: 0.13840 
 train loss: 46.74181365966797, len train_loader: 601 
Epoch: 160 | Train loss: 0.07777 | Test loss: 0.08752 
 train loss: 45.45629119873047, len train_loader: 601 
Epoch: 170 | Train loss: 0.07563 | Test loss: 0.08888 
 train loss: 44.290985107421875, len train_loader: 601 
Epoch: 180 | Train loss: 0.07370 | Test loss: 0.08718 
 train loss: 43.04212951660156, len train_loader: 601 
Epoch: 190 | Train loss: 0.07162 | Test loss: 0.08852 
 train loss: 41.57270050048828, len train_loader: 601 
Epoch: 200 | Train loss: 0.06917 | Test loss: 0.08812 
 train loss: 40.58433151245117, len train_loader: 601 
Epoch: 210 | Train loss: 0.06753 | Test loss: 0.08357 
 train loss: 39.419891357421875, len train_loader: 601 
Epoch: 220 | Train loss: 0.06559 | Test loss: 0.08406 
 train loss: 38.256195068359375, len train_loader: 601 
Epoch: 230 | Train loss: 0.06365 | Test loss: 0.07567 
 train loss: 37.64606857299805, len train_loader: 601 
Epoch: 240 | Train loss: 0.06264 | Test loss: 0.07495 
 train loss: 36.30268096923828, len train_loader: 601 
Epoch: 250 | Train loss: 0.06040 | Test loss: 0.09337 
 train loss: 35.70145797729492, len train_loader: 601 
Epoch: 260 | Train loss: 0.05940 | Test loss: 0.07887 
 train loss: 34.746063232421875, len train_loader: 601 
Epoch: 270 | Train loss: 0.05781 | Test loss: 0.09110 
 train loss: 33.826255798339844, len train_loader: 601 
Epoch: 280 | Train loss: 0.05628 | Test loss: 0.08506 
 train loss: 32.92827224731445, len train_loader: 601 
Epoch: 290 | Train loss: 0.05479 | Test loss: 0.09441 
 train loss: 32.519344329833984, len train_loader: 601 
Epoch: 300 | Train loss: 0.05411 | Test loss: 0.08438 
 train loss: 31.52159309387207, len train_loader: 601 
Epoch: 310 | Train loss: 0.05245 | Test loss: 0.08274 
 train loss: 30.77210235595703, len train_loader: 601 
Epoch: 320 | Train loss: 0.05120 | Test loss: 0.07423 
 train loss: 30.376529693603516, len train_loader: 601 
Epoch: 330 | Train loss: 0.05054 | Test loss: 0.07611 
 train loss: 29.271991729736328, len train_loader: 601 
Epoch: 340 | Train loss: 0.04871 | Test loss: 0.07635 
 train loss: 28.843137741088867, len train_loader: 601 
Epoch: 350 | Train loss: 0.04799 | Test loss: 0.08052 
 train loss: 28.178152084350586, len train_loader: 601 
Epoch: 360 | Train loss: 0.04689 | Test loss: 0.07029 
 train loss: 27.629695892333984, len train_loader: 601 
Epoch: 370 | Train loss: 0.04597 | Test loss: 0.08017 
 train loss: 27.18057632446289, len train_loader: 601 
Epoch: 380 | Train loss: 0.04523 | Test loss: 0.07127 
 train loss: 26.70755958557129, len train_loader: 601 
Epoch: 390 | Train loss: 0.04444 | Test loss: 0.07878 
 train loss: 26.00020408630371, len train_loader: 601 
Epoch: 400 | Train loss: 0.04326 | Test loss: 0.07066 
 train loss: 25.614425659179688, len train_loader: 601 
Epoch: 410 | Train loss: 0.04262 | Test loss: 0.07440 
 train loss: 25.16010093688965, len train_loader: 601 
Epoch: 420 | Train loss: 0.04186 | Test loss: 0.07322 
 train loss: 24.68345069885254, len train_loader: 601 
Epoch: 430 | Train loss: 0.04107 | Test loss: 0.07451 
 train loss: 24.244754791259766, len train_loader: 601 
Epoch: 440 | Train loss: 0.04034 | Test loss: 0.06812 
 train loss: 23.97650146484375, len train_loader: 601 
Epoch: 450 | Train loss: 0.03989 | Test loss: 0.08048 
 train loss: 23.67462921142578, len train_loader: 601 
Epoch: 460 | Train loss: 0.03939 | Test loss: 0.06878 
 train loss: 22.87616729736328, len train_loader: 601 
Epoch: 470 | Train loss: 0.03806 | Test loss: 0.06775 
 train loss: 22.846071243286133, len train_loader: 601 
Epoch: 480 | Train loss: 0.03801 | Test loss: 0.06715 
 train loss: 22.372438430786133, len train_loader: 601 
Epoch: 490 | Train loss: 0.03723 | Test loss: 0.07123 
 train loss: 21.811216354370117, len train_loader: 601 
Epoch: 500 | Train loss: 0.03629 | Test loss: 0.06889 
 train loss: 21.517202377319336, len train_loader: 601 
Epoch: 510 | Train loss: 0.03580 | Test loss: 0.07396 
 train loss: 21.242612838745117, len train_loader: 601 
Epoch: 520 | Train loss: 0.03535 | Test loss: 0.06616 
 train loss: 21.316932678222656, len train_loader: 601 
Epoch: 530 | Train loss: 0.03547 | Test loss: 0.07569 
 train loss: 20.469545364379883, len train_loader: 601 
Epoch: 540 | Train loss: 0.03406 | Test loss: 0.08298 
 train loss: 20.556936264038086, len train_loader: 601 
Epoch: 550 | Train loss: 0.03420 | Test loss: 0.06591 
 train loss: 19.98331069946289, len train_loader: 601 
Epoch: 560 | Train loss: 0.03325 | Test loss: 0.07727 
 train loss: 19.6236572265625, len train_loader: 601 
Epoch: 570 | Train loss: 0.03265 | Test loss: 0.07155 
 train loss: 19.120281219482422, len train_loader: 601 
Epoch: 580 | Train loss: 0.03181 | Test loss: 0.07218 
 train loss: 19.147077560424805, len train_loader: 601 
Epoch: 590 | Train loss: 0.03186 | Test loss: 0.07685 
 train loss: 18.73836326599121, len train_loader: 601 
Epoch: 600 | Train loss: 0.03118 | Test loss: 0.07153 
 train loss: 18.825271606445312, len train_loader: 601 
Epoch: 610 | Train loss: 0.03132 | Test loss: 0.07663 
 train loss: 18.485029220581055, len train_loader: 601 
Epoch: 620 | Train loss: 0.03076 | Test loss: 0.06784 
 train loss: 17.976394653320312, len train_loader: 601 
Epoch: 630 | Train loss: 0.02991 | Test loss: 0.06867 
 train loss: 17.89272689819336, len train_loader: 601 
Epoch: 640 | Train loss: 0.02977 | Test loss: 0.06649 
 train loss: 17.60357093811035, len train_loader: 601 
Epoch: 650 | Train loss: 0.02929 | Test loss: 0.06933 
 train loss: 17.478961944580078, len train_loader: 601 
Epoch: 660 | Train loss: 0.02908 | Test loss: 0.07231 
 train loss: 17.23251724243164, len train_loader: 601 
Epoch: 670 | Train loss: 0.02867 | Test loss: 0.06833 
 train loss: 17.089859008789062, len train_loader: 601 
Epoch: 680 | Train loss: 0.02844 | Test loss: 0.06910 
 train loss: 16.91982650756836, len train_loader: 601 
Epoch: 690 | Train loss: 0.02815 | Test loss: 0.07104 
 train loss: 16.489591598510742, len train_loader: 601 
Epoch: 700 | Train loss: 0.02744 | Test loss: 0.07994 
 train loss: 16.246389389038086, len train_loader: 601 
Epoch: 710 | Train loss: 0.02703 | Test loss: 0.06818 
 train loss: 16.15174674987793, len train_loader: 601 
Epoch: 720 | Train loss: 0.02687 | Test loss: 0.06800 
 train loss: 16.132164001464844, len train_loader: 601 
Epoch: 730 | Train loss: 0.02684 | Test loss: 0.06698 
 train loss: 16.167625427246094, len train_loader: 601 
Epoch: 740 | Train loss: 0.02690 | Test loss: 0.06697 
 train loss: 15.450101852416992, len train_loader: 601 
Epoch: 750 | Train loss: 0.02571 | Test loss: 0.07046 
 train loss: 15.572566986083984, len train_loader: 601 
Epoch: 760 | Train loss: 0.02591 | Test loss: 0.06903 
 train loss: 15.02756118774414, len train_loader: 601 
Epoch: 770 | Train loss: 0.02500 | Test loss: 0.07255 
 train loss: 14.96767520904541, len train_loader: 601 
Epoch: 780 | Train loss: 0.02490 | Test loss: 0.07279 
 train loss: 14.758846282958984, len train_loader: 601 
Epoch: 790 | Train loss: 0.02456 | Test loss: 0.06869 
 train loss: 14.752017974853516, len train_loader: 601 
Epoch: 800 | Train loss: 0.02455 | Test loss: 0.07507 
 train loss: 14.567336082458496, len train_loader: 601 
Epoch: 810 | Train loss: 0.02424 | Test loss: 0.07694 
 train loss: 14.729089736938477, len train_loader: 601 
Epoch: 820 | Train loss: 0.02451 | Test loss: 0.07312 
 train loss: 14.170928955078125, len train_loader: 601 
Epoch: 830 | Train loss: 0.02358 | Test loss: 0.06960 
 train loss: 13.931986808776855, len train_loader: 601 
Epoch: 840 | Train loss: 0.02318 | Test loss: 0.06624 
 train loss: 14.213200569152832, len train_loader: 601 
Epoch: 850 | Train loss: 0.02365 | Test loss: 0.06449 
 train loss: 13.762296676635742, len train_loader: 601 
Epoch: 860 | Train loss: 0.02290 | Test loss: 0.06334 
 train loss: 13.597381591796875, len train_loader: 601 
Epoch: 870 | Train loss: 0.02262 | Test loss: 0.06859 
 train loss: 13.63922119140625, len train_loader: 601 
Epoch: 880 | Train loss: 0.02269 | Test loss: 0.06675 
 train loss: 13.446844100952148, len train_loader: 601 
Epoch: 890 | Train loss: 0.02237 | Test loss: 0.06362 
 train loss: 13.385804176330566, len train_loader: 601 
Epoch: 900 | Train loss: 0.02227 | Test loss: 0.06442 
 train loss: 13.03648853302002, len train_loader: 601 
Epoch: 910 | Train loss: 0.02169 | Test loss: 0.07615 
 train loss: 12.986422538757324, len train_loader: 601 
Epoch: 920 | Train loss: 0.02161 | Test loss: 0.07072 
 train loss: 12.824928283691406, len train_loader: 601 
Epoch: 930 | Train loss: 0.02134 | Test loss: 0.06531 
 train loss: 12.702343940734863, len train_loader: 601 
Epoch: 940 | Train loss: 0.02114 | Test loss: 0.07219 
 train loss: 12.645709991455078, len train_loader: 601 
Epoch: 950 | Train loss: 0.02104 | Test loss: 0.06833 
 train loss: 12.545613288879395, len train_loader: 601 
Epoch: 960 | Train loss: 0.02087 | Test loss: 0.06289 
 train loss: 12.349196434020996, len train_loader: 601 
Epoch: 970 | Train loss: 0.02055 | Test loss: 0.06462 
 train loss: 12.439809799194336, len train_loader: 601 
Epoch: 980 | Train loss: 0.02070 | Test loss: 0.06718 
 train loss: 12.37560749053955, len train_loader: 601 
Epoch: 990 | Train loss: 0.02059 | Test loss: 0.06434 
 train loss: 11.944112777709961, len train_loader: 601 
Epoch: 1000 | Train loss: 0.01987 | Test loss: 0.06629 
 train loss: 11.923656463623047, len train_loader: 601 
Epoch: 1010 | Train loss: 0.01984 | Test loss: 0.06403 
 train loss: 11.98720932006836, len train_loader: 601 
Epoch: 1020 | Train loss: 0.01995 | Test loss: 0.06352 
 train loss: 11.560649871826172, len train_loader: 601 
Epoch: 1030 | Train loss: 0.01924 | Test loss: 0.06488 
 train loss: 11.657752990722656, len train_loader: 601 
Epoch: 1040 | Train loss: 0.01940 | Test loss: 0.06449 
 train loss: 11.425726890563965, len train_loader: 601 
Epoch: 1050 | Train loss: 0.01901 | Test loss: 0.06439 
 train loss: 11.479920387268066, len train_loader: 601 
Epoch: 1060 | Train loss: 0.01910 | Test loss: 0.06197 
 train loss: 11.308855056762695, len train_loader: 601 
Epoch: 1070 | Train loss: 0.01882 | Test loss: 0.06619 
 train loss: 11.284637451171875, len train_loader: 601 
Epoch: 1080 | Train loss: 0.01878 | Test loss: 0.06759 
 train loss: 11.22169303894043, len train_loader: 601 
Epoch: 1090 | Train loss: 0.01867 | Test loss: 0.06356 
 train loss: 11.148201942443848, len train_loader: 601 
Epoch: 1100 | Train loss: 0.01855 | Test loss: 0.06184 
 train loss: 11.10125732421875, len train_loader: 601 
Epoch: 1110 | Train loss: 0.01847 | Test loss: 0.06263 
 train loss: 10.812067985534668, len train_loader: 601 
Epoch: 1120 | Train loss: 0.01799 | Test loss: 0.06907 
 train loss: 10.996295928955078, len train_loader: 601 
Epoch: 1130 | Train loss: 0.01830 | Test loss: 0.06353 
 train loss: 10.737823486328125, len train_loader: 601 
Epoch: 1140 | Train loss: 0.01787 | Test loss: 0.06125 
 train loss: 10.551005363464355, len train_loader: 601 
Epoch: 1150 | Train loss: 0.01756 | Test loss: 0.06229 
 train loss: 10.363482475280762, len train_loader: 601 
Epoch: 1160 | Train loss: 0.01724 | Test loss: 0.06207 
 train loss: 10.667427062988281, len train_loader: 601 
Epoch: 1170 | Train loss: 0.01775 | Test loss: 0.06383 
 train loss: 10.425955772399902, len train_loader: 601 
Epoch: 1180 | Train loss: 0.01735 | Test loss: 0.06278 
 train loss: 10.432100296020508, len train_loader: 601 
Epoch: 1190 | Train loss: 0.01736 | Test loss: 0.06091 
 train loss: 10.270159721374512, len train_loader: 601 
Epoch: 1200 | Train loss: 0.01709 | Test loss: 0.06367 
 train loss: 10.326287269592285, len train_loader: 601 
Epoch: 1210 | Train loss: 0.01718 | Test loss: 0.06437 
 train loss: 10.028095245361328, len train_loader: 601 
Epoch: 1220 | Train loss: 0.01669 | Test loss: 0.06304 
 train loss: 10.15865421295166, len train_loader: 601 
Epoch: 1230 | Train loss: 0.01690 | Test loss: 0.06452 
 train loss: 9.98373794555664, len train_loader: 601 
Epoch: 1240 | Train loss: 0.01661 | Test loss: 0.06138 
 train loss: 9.890997886657715, len train_loader: 601 
Epoch: 1250 | Train loss: 0.01646 | Test loss: 0.06241 
 train loss: 9.939712524414062, len train_loader: 601 
Epoch: 1260 | Train loss: 0.01654 | Test loss: 0.06266 
 train loss: 9.769491195678711, len train_loader: 601 
Epoch: 1270 | Train loss: 0.01626 | Test loss: 0.06230 
 train loss: 9.74732780456543, len train_loader: 601 
Epoch: 1280 | Train loss: 0.01622 | Test loss: 0.06571 
 train loss: 9.568676948547363, len train_loader: 601 
Epoch: 1290 | Train loss: 0.01592 | Test loss: 0.06235 
 train loss: 9.62204360961914, len train_loader: 601 
Epoch: 1300 | Train loss: 0.01601 | Test loss: 0.06386 
 train loss: 9.312170028686523, len train_loader: 601 
Epoch: 1310 | Train loss: 0.01549 | Test loss: 0.06050 
 train loss: 9.62325668334961, len train_loader: 601 
Epoch: 1320 | Train loss: 0.01601 | Test loss: 0.06160 
 train loss: 9.251962661743164, len train_loader: 601 
Epoch: 1330 | Train loss: 0.01539 | Test loss: 0.06174 
 train loss: 9.438294410705566, len train_loader: 601 
Epoch: 1340 | Train loss: 0.01570 | Test loss: 0.06163 
 train loss: 9.24872875213623, len train_loader: 601 
Epoch: 1350 | Train loss: 0.01539 | Test loss: 0.06340 
 train loss: 9.276190757751465, len train_loader: 601 
Epoch: 1360 | Train loss: 0.01543 | Test loss: 0.06198 
 train loss: 9.261112213134766, len train_loader: 601 
Epoch: 1370 | Train loss: 0.01541 | Test loss: 0.06185 
 train loss: 9.068746566772461, len train_loader: 601 
Epoch: 1380 | Train loss: 0.01509 | Test loss: 0.06185 
 train loss: 9.113068580627441, len train_loader: 601 
Epoch: 1390 | Train loss: 0.01516 | Test loss: 0.05970 
 train loss: 8.978669166564941, len train_loader: 601 
Epoch: 1400 | Train loss: 0.01494 | Test loss: 0.06605 
 train loss: 8.922155380249023, len train_loader: 601 
Epoch: 1410 | Train loss: 0.01485 | Test loss: 0.06136 
 train loss: 8.765277862548828, len train_loader: 601 
Epoch: 1420 | Train loss: 0.01458 | Test loss: 0.06064 
 train loss: 8.886290550231934, len train_loader: 601 
Epoch: 1430 | Train loss: 0.01479 | Test loss: 0.06197 
 train loss: 8.666921615600586, len train_loader: 601 
Epoch: 1440 | Train loss: 0.01442 | Test loss: 0.06055 
 train loss: 8.732182502746582, len train_loader: 601 
Epoch: 1450 | Train loss: 0.01453 | Test loss: 0.06145 
 train loss: 8.626059532165527, len train_loader: 601 
Epoch: 1460 | Train loss: 0.01435 | Test loss: 0.06140 
 train loss: 8.639408111572266, len train_loader: 601 
Epoch: 1470 | Train loss: 0.01438 | Test loss: 0.06142 
 train loss: 8.480463981628418, len train_loader: 601 
Epoch: 1480 | Train loss: 0.01411 | Test loss: 0.06272 
 train loss: 8.498125076293945, len train_loader: 601 
Epoch: 1490 | Train loss: 0.01414 | Test loss: 0.06211 
 train loss: 8.488421440124512, len train_loader: 601 
Epoch: 1500 | Train loss: 0.01412 | Test loss: 0.06589 
 train loss: 8.299555778503418, len train_loader: 601 
Epoch: 1510 | Train loss: 0.01381 | Test loss: 0.05994 
 train loss: 8.359711647033691, len train_loader: 601 
Epoch: 1520 | Train loss: 0.01391 | Test loss: 0.06101 
 train loss: 8.268288612365723, len train_loader: 601 
Epoch: 1530 | Train loss: 0.01376 | Test loss: 0.06080 
 train loss: 8.293204307556152, len train_loader: 601 
Epoch: 1540 | Train loss: 0.01380 | Test loss: 0.05984 
 train loss: 8.244210243225098, len train_loader: 601 
Epoch: 1550 | Train loss: 0.01372 | Test loss: 0.06099 
 train loss: 8.225769996643066, len train_loader: 601 
Epoch: 1560 | Train loss: 0.01369 | Test loss: 0.06134 
 train loss: 8.075420379638672, len train_loader: 601 
Epoch: 1570 | Train loss: 0.01344 | Test loss: 0.06001 
 train loss: 8.181449890136719, len train_loader: 601 
Epoch: 1580 | Train loss: 0.01361 | Test loss: 0.05988 
 train loss: 7.973430156707764, len train_loader: 601 
Epoch: 1590 | Train loss: 0.01327 | Test loss: 0.06090 
 train loss: 8.06984806060791, len train_loader: 601 
Epoch: 1600 | Train loss: 0.01343 | Test loss: 0.06019 
 train loss: 7.878138065338135, len train_loader: 601 
Epoch: 1610 | Train loss: 0.01311 | Test loss: 0.06091 
 train loss: 8.090078353881836, len train_loader: 601 
Epoch: 1620 | Train loss: 0.01346 | Test loss: 0.05955 
 train loss: 7.769503116607666, len train_loader: 601 
Epoch: 1630 | Train loss: 0.01293 | Test loss: 0.06014 
 train loss: 7.946177005767822, len train_loader: 601 
Epoch: 1640 | Train loss: 0.01322 | Test loss: 0.06146 
 train loss: 7.871927261352539, len train_loader: 601 
Epoch: 1650 | Train loss: 0.01310 | Test loss: 0.05971 
 train loss: 7.787740230560303, len train_loader: 601 
Epoch: 1660 | Train loss: 0.01296 | Test loss: 0.06036 
 train loss: 7.889996528625488, len train_loader: 601 
Epoch: 1670 | Train loss: 0.01313 | Test loss: 0.05929 
 train loss: 7.7712554931640625, len train_loader: 601 
Epoch: 1680 | Train loss: 0.01293 | Test loss: 0.06058 
 train loss: 7.56771993637085, len train_loader: 601 
Epoch: 1690 | Train loss: 0.01259 | Test loss: 0.06135 
 train loss: 7.641724586486816, len train_loader: 601 
Epoch: 1700 | Train loss: 0.01272 | Test loss: 0.06035 
 train loss: 7.514075756072998, len train_loader: 601 
Epoch: 1710 | Train loss: 0.01250 | Test loss: 0.05975 
 train loss: 7.667449474334717, len train_loader: 601 
Epoch: 1720 | Train loss: 0.01276 | Test loss: 0.06103 
 train loss: 7.473759651184082, len train_loader: 601 
Epoch: 1730 | Train loss: 0.01244 | Test loss: 0.06274 
 train loss: 7.456367015838623, len train_loader: 601 
Epoch: 1740 | Train loss: 0.01241 | Test loss: 0.05950 
 train loss: 7.3484039306640625, len train_loader: 601 
Epoch: 1750 | Train loss: 0.01223 | Test loss: 0.06067 
 train loss: 7.502464771270752, len train_loader: 601 
Epoch: 1760 | Train loss: 0.01248 | Test loss: 0.06102 
 train loss: 7.345654487609863, len train_loader: 601 
Epoch: 1770 | Train loss: 0.01222 | Test loss: 0.06177 
 train loss: 7.4933953285217285, len train_loader: 601 
Epoch: 1780 | Train loss: 0.01247 | Test loss: 0.06130 
 train loss: 7.326107501983643, len train_loader: 601 
Epoch: 1790 | Train loss: 0.01219 | Test loss: 0.05894 
 train loss: 7.250016212463379, len train_loader: 601 
Epoch: 1800 | Train loss: 0.01206 | Test loss: 0.05992 
 train loss: 7.2310333251953125, len train_loader: 601 
Epoch: 1810 | Train loss: 0.01203 | Test loss: 0.06104 
 train loss: 7.2068963050842285, len train_loader: 601 
Epoch: 1820 | Train loss: 0.01199 | Test loss: 0.05967 
 train loss: 7.261244297027588, len train_loader: 601 
Epoch: 1830 | Train loss: 0.01208 | Test loss: 0.06017 
 train loss: 7.21779727935791, len train_loader: 601 
Epoch: 1840 | Train loss: 0.01201 | Test loss: 0.05972 
 train loss: 7.152982234954834, len train_loader: 601 
Epoch: 1850 | Train loss: 0.01190 | Test loss: 0.06032 
 train loss: 6.982396125793457, len train_loader: 601 
Epoch: 1860 | Train loss: 0.01162 | Test loss: 0.06274 
 train loss: 7.064568042755127, len train_loader: 601 
Epoch: 1870 | Train loss: 0.01175 | Test loss: 0.06102 
 train loss: 7.073432445526123, len train_loader: 601 
Epoch: 1880 | Train loss: 0.01177 | Test loss: 0.05997 
 train loss: 6.979839324951172, len train_loader: 601 
Epoch: 1890 | Train loss: 0.01161 | Test loss: 0.06031 
 train loss: 6.97098970413208, len train_loader: 601 
Epoch: 1900 | Train loss: 0.01160 | Test loss: 0.05935 
 train loss: 6.926006317138672, len train_loader: 601 
Epoch: 1910 | Train loss: 0.01152 | Test loss: 0.06009 
 train loss: 6.958425998687744, len train_loader: 601 
Epoch: 1920 | Train loss: 0.01158 | Test loss: 0.06090 
 train loss: 6.867916107177734, len train_loader: 601 
Epoch: 1930 | Train loss: 0.01143 | Test loss: 0.06413 
 train loss: 6.844265937805176, len train_loader: 601 
Epoch: 1940 | Train loss: 0.01139 | Test loss: 0.06271 
 train loss: 6.800065994262695, len train_loader: 601 
Epoch: 1950 | Train loss: 0.01131 | Test loss: 0.05988 
 train loss: 6.899954795837402, len train_loader: 601 
Epoch: 1960 | Train loss: 0.01148 | Test loss: 0.05930 
 train loss: 6.905282974243164, len train_loader: 601 
Epoch: 1970 | Train loss: 0.01149 | Test loss: 0.06332 
 train loss: 6.861942768096924, len train_loader: 601 
Epoch: 1980 | Train loss: 0.01142 | Test loss: 0.06342 
 train loss: 6.842532634735107, len train_loader: 601 
Epoch: 1990 | Train loss: 0.01139 | Test loss: 0.06315 
 train loss: 6.73345947265625, len train_loader: 601 
Epoch: 2000 | Train loss: 0.01120 | Test loss: 0.05991 
 train loss: 6.703423023223877, len train_loader: 601 
Epoch: 2010 | Train loss: 0.01115 | Test loss: 0.05953 
 train loss: 6.728276252746582, len train_loader: 601 
Epoch: 2020 | Train loss: 0.01120 | Test loss: 0.05960 
 train loss: 6.6439900398254395, len train_loader: 601 
Epoch: 2030 | Train loss: 0.01105 | Test loss: 0.06037 
 train loss: 6.726495265960693, len train_loader: 601 
Epoch: 2040 | Train loss: 0.01119 | Test loss: 0.06280 
 train loss: 6.573932647705078, len train_loader: 601 
Epoch: 2050 | Train loss: 0.01094 | Test loss: 0.06115 
 train loss: 6.609418869018555, len train_loader: 601 
Epoch: 2060 | Train loss: 0.01100 | Test loss: 0.05883 
 train loss: 6.617375373840332, len train_loader: 601 
Epoch: 2070 | Train loss: 0.01101 | Test loss: 0.05907 
 train loss: 6.4803032875061035, len train_loader: 601 
Epoch: 2080 | Train loss: 0.01078 | Test loss: 0.05897 
 train loss: 6.5593132972717285, len train_loader: 601 
Epoch: 2090 | Train loss: 0.01091 | Test loss: 0.05923 
 train loss: 6.399215221405029, len train_loader: 601 
Epoch: 2100 | Train loss: 0.01065 | Test loss: 0.05867 
 train loss: 6.539242267608643, len train_loader: 601 
Epoch: 2110 | Train loss: 0.01088 | Test loss: 0.05944 
 train loss: 6.642796993255615, len train_loader: 601 
Epoch: 2120 | Train loss: 0.01105 | Test loss: 0.05956 
 train loss: 6.484095096588135, len train_loader: 601 
Epoch: 2130 | Train loss: 0.01079 | Test loss: 0.05885 
 train loss: 6.524553298950195, len train_loader: 601 
Epoch: 2140 | Train loss: 0.01086 | Test loss: 0.06032 
 train loss: 6.419160842895508, len train_loader: 601 
Epoch: 2150 | Train loss: 0.01068 | Test loss: 0.05895 
 train loss: 6.410287857055664, len train_loader: 601 
Epoch: 2160 | Train loss: 0.01067 | Test loss: 0.05905 
 train loss: 6.366955757141113, len train_loader: 601 
Epoch: 2170 | Train loss: 0.01059 | Test loss: 0.06042 
 train loss: 6.401777267456055, len train_loader: 601 
Epoch: 2180 | Train loss: 0.01065 | Test loss: 0.06031 
 train loss: 6.27902364730835, len train_loader: 601 
Epoch: 2190 | Train loss: 0.01045 | Test loss: 0.06162 
 train loss: 6.277216911315918, len train_loader: 601 
Epoch: 2200 | Train loss: 0.01044 | Test loss: 0.06192 
 train loss: 6.3204450607299805, len train_loader: 601 
Epoch: 2210 | Train loss: 0.01052 | Test loss: 0.05853 
 train loss: 6.2435479164123535, len train_loader: 601 
Epoch: 2220 | Train loss: 0.01039 | Test loss: 0.05847 
 train loss: 6.230802059173584, len train_loader: 601 
Epoch: 2230 | Train loss: 0.01037 | Test loss: 0.05988 
 train loss: 6.237572193145752, len train_loader: 601 
Epoch: 2240 | Train loss: 0.01038 | Test loss: 0.05930 
 train loss: 6.1337056159973145, len train_loader: 601 
Epoch: 2250 | Train loss: 0.01021 | Test loss: 0.05906 
 train loss: 6.124669075012207, len train_loader: 601 
Epoch: 2260 | Train loss: 0.01019 | Test loss: 0.05854 
 train loss: 6.17832612991333, len train_loader: 601 
Epoch: 2270 | Train loss: 0.01028 | Test loss: 0.06228 
 train loss: 6.178534030914307, len train_loader: 601 
Epoch: 2280 | Train loss: 0.01028 | Test loss: 0.05852 
 train loss: 6.102367401123047, len train_loader: 601 
Epoch: 2290 | Train loss: 0.01015 | Test loss: 0.05868 
 train loss: 6.100478649139404, len train_loader: 601 
Epoch: 2300 | Train loss: 0.01015 | Test loss: 0.05966 
 train loss: 6.104686260223389, len train_loader: 601 
Epoch: 2310 | Train loss: 0.01016 | Test loss: 0.06214 
 train loss: 6.076959609985352, len train_loader: 601 
Epoch: 2320 | Train loss: 0.01011 | Test loss: 0.05903 
 train loss: 5.995574474334717, len train_loader: 601 
Epoch: 2330 | Train loss: 0.00998 | Test loss: 0.05943 
 train loss: 6.096166610717773, len train_loader: 601 
Epoch: 2340 | Train loss: 0.01014 | Test loss: 0.06121 
 train loss: 6.0920023918151855, len train_loader: 601 
Epoch: 2350 | Train loss: 0.01014 | Test loss: 0.05917 
 train loss: 6.062897205352783, len train_loader: 601 
Epoch: 2360 | Train loss: 0.01009 | Test loss: 0.05863 
 train loss: 6.028887748718262, len train_loader: 601 
Epoch: 2370 | Train loss: 0.01003 | Test loss: 0.05944 
 train loss: 6.023634433746338, len train_loader: 601 
Epoch: 2380 | Train loss: 0.01002 | Test loss: 0.05866 
 train loss: 5.947057247161865, len train_loader: 601 
Epoch: 2390 | Train loss: 0.00990 | Test loss: 0.05902 
 train loss: 5.97210168838501, len train_loader: 601 
Epoch: 2400 | Train loss: 0.00994 | Test loss: 0.05820 
 train loss: 5.875492095947266, len train_loader: 601 
Epoch: 2410 | Train loss: 0.00978 | Test loss: 0.05929 
 train loss: 5.935557842254639, len train_loader: 601 
Epoch: 2420 | Train loss: 0.00988 | Test loss: 0.05853 
 train loss: 5.825194835662842, len train_loader: 601 
Epoch: 2430 | Train loss: 0.00969 | Test loss: 0.06125 
 train loss: 5.839725494384766, len train_loader: 601 
Epoch: 2440 | Train loss: 0.00972 | Test loss: 0.05919 
 train loss: 5.880062580108643, len train_loader: 601 
Epoch: 2450 | Train loss: 0.00978 | Test loss: 0.05864 
 train loss: 5.882033824920654, len train_loader: 601 
Epoch: 2460 | Train loss: 0.00979 | Test loss: 0.06110 
 train loss: 5.874446392059326, len train_loader: 601 
Epoch: 2470 | Train loss: 0.00977 | Test loss: 0.05803 
 train loss: 5.875012397766113, len train_loader: 601 
Epoch: 2480 | Train loss: 0.00978 | Test loss: 0.05770 
 train loss: 5.806439399719238, len train_loader: 601 
Epoch: 2490 | Train loss: 0.00966 | Test loss: 0.05902 
 train loss: 5.729915142059326, len train_loader: 601 
Epoch: 2500 | Train loss: 0.00953 | Test loss: 0.05965 
 train loss: 5.765291213989258, len train_loader: 601 
Epoch: 2510 | Train loss: 0.00959 | Test loss: 0.05933 
 train loss: 5.762109756469727, len train_loader: 601 
Epoch: 2520 | Train loss: 0.00959 | Test loss: 0.06052 
 train loss: 5.762075901031494, len train_loader: 601 
Epoch: 2530 | Train loss: 0.00959 | Test loss: 0.05938 
 train loss: 5.704394340515137, len train_loader: 601 
Epoch: 2540 | Train loss: 0.00949 | Test loss: 0.06097 
 train loss: 5.754594802856445, len train_loader: 601 
Epoch: 2550 | Train loss: 0.00958 | Test loss: 0.05847 
 train loss: 5.6582441329956055, len train_loader: 601 
Epoch: 2560 | Train loss: 0.00941 | Test loss: 0.05857 
 train loss: 5.74260950088501, len train_loader: 601 
Epoch: 2570 | Train loss: 0.00956 | Test loss: 0.05905 
 train loss: 5.547010898590088, len train_loader: 601 
Epoch: 2580 | Train loss: 0.00923 | Test loss: 0.05863 
 train loss: 5.712630748748779, len train_loader: 601 
Epoch: 2590 | Train loss: 0.00951 | Test loss: 0.05800 
 train loss: 5.6794209480285645, len train_loader: 601 
Epoch: 2600 | Train loss: 0.00945 | Test loss: 0.06019 
 train loss: 5.6439948081970215, len train_loader: 601 
Epoch: 2610 | Train loss: 0.00939 | Test loss: 0.05891 
 train loss: 5.5927414894104, len train_loader: 601 
Epoch: 2620 | Train loss: 0.00931 | Test loss: 0.05879 
 train loss: 5.518173694610596, len train_loader: 601 
Epoch: 2630 | Train loss: 0.00918 | Test loss: 0.05866 
 train loss: 5.602399826049805, len train_loader: 601 
Epoch: 2640 | Train loss: 0.00932 | Test loss: 0.05843 
 train loss: 5.564330577850342, len train_loader: 601 
Epoch: 2650 | Train loss: 0.00926 | Test loss: 0.05873 
 train loss: 5.572999954223633, len train_loader: 601 
Epoch: 2660 | Train loss: 0.00927 | Test loss: 0.05916 
 train loss: 5.559639930725098, len train_loader: 601 
Epoch: 2670 | Train loss: 0.00925 | Test loss: 0.06010 
 train loss: 5.478160858154297, len train_loader: 601 
Epoch: 2680 | Train loss: 0.00912 | Test loss: 0.06222 
 train loss: 5.541532039642334, len train_loader: 601 
Epoch: 2690 | Train loss: 0.00922 | Test loss: 0.05845 
 train loss: 5.6113433837890625, len train_loader: 601 
Epoch: 2700 | Train loss: 0.00934 | Test loss: 0.05759 
 train loss: 5.450417518615723, len train_loader: 601 
Epoch: 2710 | Train loss: 0.00907 | Test loss: 0.05880 
 train loss: 5.528517723083496, len train_loader: 601 
Epoch: 2720 | Train loss: 0.00920 | Test loss: 0.05842 
 train loss: 5.49659538269043, len train_loader: 601 
Epoch: 2730 | Train loss: 0.00915 | Test loss: 0.05920 
 train loss: 5.4028639793396, len train_loader: 601 
Epoch: 2740 | Train loss: 0.00899 | Test loss: 0.05920 
 train loss: 5.377222537994385, len train_loader: 601 
Epoch: 2750 | Train loss: 0.00895 | Test loss: 0.05882 
 train loss: 5.446063041687012, len train_loader: 601 
Epoch: 2760 | Train loss: 0.00906 | Test loss: 0.05907 
 train loss: 5.41159200668335, len train_loader: 601 
Epoch: 2770 | Train loss: 0.00900 | Test loss: 0.05967 
 train loss: 5.39676570892334, len train_loader: 601 
Epoch: 2780 | Train loss: 0.00898 | Test loss: 0.06194 
 train loss: 5.5006513595581055, len train_loader: 601 
Epoch: 2790 | Train loss: 0.00915 | Test loss: 0.05864 
 train loss: 5.391644477844238, len train_loader: 601 
Epoch: 2800 | Train loss: 0.00897 | Test loss: 0.06031 
 train loss: 5.435624599456787, len train_loader: 601 
Epoch: 2810 | Train loss: 0.00904 | Test loss: 0.05884 
 train loss: 5.3735151290893555, len train_loader: 601 
Epoch: 2820 | Train loss: 0.00894 | Test loss: 0.05894 
 train loss: 5.395919322967529, len train_loader: 601 
Epoch: 2830 | Train loss: 0.00898 | Test loss: 0.05825 
 train loss: 5.258133411407471, len train_loader: 601 
Epoch: 2840 | Train loss: 0.00875 | Test loss: 0.05761 
 train loss: 5.24077033996582, len train_loader: 601 
Epoch: 2850 | Train loss: 0.00872 | Test loss: 0.05897 
 train loss: 5.17135763168335, len train_loader: 601 
Epoch: 2860 | Train loss: 0.00860 | Test loss: 0.05763 
 train loss: 5.252061367034912, len train_loader: 601 
Epoch: 2870 | Train loss: 0.00874 | Test loss: 0.05854 
 train loss: 5.2363481521606445, len train_loader: 601 
Epoch: 2880 | Train loss: 0.00871 | Test loss: 0.05877 
 train loss: 5.293179988861084, len train_loader: 601 
Epoch: 2890 | Train loss: 0.00881 | Test loss: 0.05797 
 train loss: 5.2017059326171875, len train_loader: 601 
Epoch: 2900 | Train loss: 0.00866 | Test loss: 0.06059 
 train loss: 5.217306137084961, len train_loader: 601 
Epoch: 2910 | Train loss: 0.00868 | Test loss: 0.05870 
 train loss: 5.251136779785156, len train_loader: 601 
Epoch: 2920 | Train loss: 0.00874 | Test loss: 0.05921 
 train loss: 5.226155757904053, len train_loader: 601 
Epoch: 2930 | Train loss: 0.00870 | Test loss: 0.05993 
 train loss: 5.194690227508545, len train_loader: 601 
Epoch: 2940 | Train loss: 0.00864 | Test loss: 0.05830 
 train loss: 5.2652974128723145, len train_loader: 601 
Epoch: 2950 | Train loss: 0.00876 | Test loss: 0.05932 
 train loss: 5.149219989776611, len train_loader: 601 
Epoch: 2960 | Train loss: 0.00857 | Test loss: 0.05805 
 train loss: 5.237423896789551, len train_loader: 601 
Epoch: 2970 | Train loss: 0.00871 | Test loss: 0.05881 
 train loss: 5.162471294403076, len train_loader: 601 
Epoch: 2980 | Train loss: 0.00859 | Test loss: 0.06136 
 train loss: 5.132850646972656, len train_loader: 601 
Epoch: 2990 | Train loss: 0.00854 | Test loss: 0.05858 
 state dict del modello: OrderedDict([('linear_layer_stack.0.weight', tensor([[ 0.0234, -0.0204,  0.0236,  ..., -0.0057, -0.0917, -0.0018],
        [-0.0230,  0.0225, -0.0113,  ...,  0.0006, -0.0070,  0.0085],
        [-0.0093,  0.0009, -0.0138,  ..., -0.0041,  0.0054,  0.0170],
        ...,
        [ 0.0125,  0.0028,  0.0025,  ..., -0.0390,  0.1832, -0.0163],
        [-0.0239,  0.0180,  0.0067,  ..., -0.0029, -0.0261, -0.0296],
        [ 0.0187, -0.0101,  0.0153,  ...,  0.0114, -0.0070,  0.0089]])), ('linear_layer_stack.0.bias', tensor([-2.3177e-03, -1.9349e-02, -1.9905e-02, -6.5450e-03, -1.0077e-02,
        -2.4473e-02,  7.9288e-03, -1.5659e-02,  1.4636e-02,  2.5635e-02,
        -1.5705e-02,  4.7999e-03,  2.7352e-04, -9.7009e-04,  1.4674e-02,
        -8.5195e-03,  2.3753e-02, -2.2351e-02, -2.3400e-02, -7.6507e-03,
         1.0910e-02,  1.4333e-03,  1.1651e-02,  2.2384e-02,  1.1693e-02,
        -1.2418e-02,  2.2304e-02,  1.0782e-02, -8.5022e-04,  1.6728e-02,
         2.0562e-02, -1.8178e-02,  2.2893e-03,  2.0719e-02, -1.8725e-02,
         2.4088e-02, -2.3634e-02, -1.6090e-02, -5.8391e-03,  1.3979e-02,
        -2.4211e-02,  4.5945e-03, -2.1600e-02, -7.2361e-03,  1.5828e-02,
        -1.4758e-02, -3.7730e-03,  2.5017e-03,  1.3660e-02,  1.2801e-02,
        -6.2525e-03,  7.9281e-03, -1.4453e-03, -1.1518e-02, -1.1403e-02,
         1.6817e-02,  2.3879e-03, -2.7675e-03, -9.5213e-03, -2.7537e-02,
        -2.5908e-02,  1.2430e-02,  1.1043e-02, -2.8957e-03, -2.0278e-02,
        -1.8763e-02, -7.0509e-03,  2.6785e-02,  2.2694e-02,  1.6430e-02,
         2.6139e-02, -1.0730e-02,  6.6482e-03, -1.9240e-02, -2.6089e-02,
         2.3787e-02,  1.3054e-02,  6.8747e-03, -1.4287e-02,  2.7069e-02,
         2.0201e-02, -4.1430e-03,  1.8812e-02,  1.3647e-02, -7.2587e-03,
        -2.5410e-02,  3.7478e-03,  9.4646e-03,  2.4170e-03,  1.8137e-02,
         1.1374e-02, -1.1306e-02, -2.2941e-02,  5.7476e-03,  2.1789e-02,
         2.8619e-03,  4.3474e-03, -1.1540e-02,  1.9383e-02, -2.3025e-02,
         1.5878e-02,  3.3521e-04,  1.8915e-02, -5.7096e-03, -2.0363e-02,
         7.3232e-03, -1.0422e-02, -2.2767e-02, -5.4834e-03,  2.1038e-03,
        -2.2191e-02,  1.0035e-02, -5.4824e-03, -1.9518e-05,  1.1545e-03,
        -9.9645e-03,  2.1932e-02,  2.2361e-02,  2.1079e-02,  2.1601e-02,
        -1.8868e-02, -2.3821e-02, -4.8088e-03, -8.5581e-03,  9.7669e-03,
        -6.7370e-03,  9.7065e-03,  2.1934e-02, -9.9500e-03, -2.5371e-02,
        -2.4155e-02,  1.5020e-02, -1.1228e-02,  6.9665e-04,  2.1344e-02,
         2.5494e-02,  2.0806e-02,  1.2457e-03, -2.5517e-02, -2.4253e-03,
        -1.0281e-03, -8.7321e-03,  8.6665e-03,  1.9495e-02, -2.2621e-02,
        -2.7463e-02,  2.2117e-03,  7.1901e-03, -1.1585e-02, -4.3053e-03,
        -1.5752e-02, -2.5354e-02,  1.3326e-02, -2.1682e-02,  5.5974e-03,
         1.5171e-02, -1.7640e-02, -1.0372e-02, -4.9914e-03, -1.6394e-02,
         2.0328e-02,  2.2307e-02, -1.4806e-02, -1.9363e-02, -1.1695e-02,
        -6.1725e-03, -1.6128e-02,  1.2299e-02, -1.5141e-03, -1.6657e-02,
         3.3263e-04, -3.2571e-03, -2.0947e-02,  8.6228e-03, -1.6437e-02,
        -1.1342e-02, -2.1300e-02, -1.4545e-02, -2.0200e-02, -1.9377e-02,
         1.2855e-02, -5.2820e-03, -2.4835e-02,  1.5231e-02, -1.9765e-02,
        -2.4515e-02,  1.6878e-02,  1.5391e-02,  1.2037e-02, -6.0718e-04,
         6.7125e-03,  1.4516e-02,  1.4017e-02,  1.9948e-02,  9.6518e-04,
         1.1370e-02,  7.6616e-03, -1.3186e-02,  1.6352e-02,  7.5924e-03,
         4.6043e-03,  9.6962e-03, -4.8765e-03,  1.3012e-02,  2.3330e-02,
        -2.1605e-02, -1.1725e-02,  1.3615e-02, -2.2457e-02, -8.6922e-03,
        -2.3922e-02, -3.7137e-03,  4.2547e-03, -2.5340e-02, -1.4637e-02,
         3.0205e-03,  7.2753e-04,  6.3625e-03,  4.7372e-03,  8.2874e-03,
         2.2482e-02,  2.1123e-02, -2.3722e-02, -1.5149e-04, -1.8900e-02,
        -2.9613e-04, -2.0480e-02, -1.8646e-02,  5.4729e-03, -2.4957e-03,
         8.6264e-03, -6.0035e-03, -1.1617e-02, -7.1303e-03, -1.2134e-02,
        -1.2629e-02, -2.1611e-02, -8.7463e-03,  8.8042e-03,  2.1358e-02,
         1.0262e-02,  2.3641e-02, -6.1382e-03, -1.8841e-02, -5.7400e-03,
         1.1387e-02,  4.6623e-03,  9.8693e-03,  1.3709e-02, -1.5561e-02,
        -1.0218e-02,  2.1900e-02, -4.7873e-03, -5.0347e-03, -1.8912e-02,
         3.5495e-03,  2.0548e-02, -2.0272e-02, -6.9833e-03, -2.3945e-02,
        -1.1625e-02,  1.2972e-02, -2.8930e-02,  2.3797e-02, -1.6452e-02,
        -2.1332e-02,  1.8732e-02,  2.2803e-02,  1.5013e-02,  8.7445e-03,
        -1.2057e-02,  1.8737e-02,  2.4136e-02, -7.1102e-03, -8.3874e-04,
         1.4863e-02,  2.3755e-02,  2.3639e-02,  1.6585e-03,  1.3709e-02,
         1.0625e-02,  2.1032e-02,  2.2185e-02, -1.7652e-02, -1.1715e-02,
        -2.6108e-02,  1.5247e-02,  5.4400e-03,  1.1367e-03,  2.4342e-02,
         2.3203e-02,  2.0491e-02,  1.0008e-02,  1.8911e-02, -1.6162e-02,
        -1.9619e-02,  1.6389e-02,  2.2046e-03, -1.3178e-03, -2.1039e-02,
         3.2024e-03, -6.9794e-03,  2.5582e-02,  1.0256e-02, -1.6938e-02,
        -1.5903e-02,  9.1667e-03,  1.7017e-03, -1.8463e-02,  4.6756e-03,
        -1.6685e-02,  1.5995e-02, -1.5493e-02,  1.6829e-02,  3.3758e-03,
        -7.3662e-03,  1.3546e-02, -1.4057e-02,  2.3353e-02, -7.4081e-03,
        -9.4005e-03, -1.3485e-03, -4.8738e-03, -1.0300e-02,  4.1567e-03,
        -1.7219e-02, -6.7072e-03, -2.4725e-02, -1.7573e-02, -8.4830e-03,
        -1.6926e-02,  2.2843e-02,  1.1086e-02, -1.8484e-02,  1.7465e-03,
        -3.4728e-04, -1.8188e-02, -4.5577e-03, -2.5577e-03,  1.0511e-02,
         1.4101e-02,  1.0771e-02,  1.0857e-02, -5.8620e-03, -1.6850e-02,
        -2.5056e-02,  1.2174e-02,  1.0177e-02,  1.6652e-02, -1.3012e-02,
         1.1208e-03,  2.0493e-02, -1.7876e-02, -1.3059e-02, -8.7978e-03,
         6.1009e-04,  1.2269e-02,  1.4973e-02,  2.4438e-02,  2.1876e-02,
        -3.8988e-03,  1.3246e-02, -2.3367e-02, -2.9699e-03,  2.0112e-02,
         3.7153e-03,  1.7859e-02, -8.1530e-03,  6.9443e-03, -2.1056e-02,
        -7.4000e-03,  1.4380e-02,  2.2116e-02,  8.1434e-03,  1.3325e-03,
        -1.0262e-02,  1.5873e-02,  2.1102e-02, -1.5568e-02, -8.7472e-03,
        -4.4804e-03,  7.3982e-03, -2.5929e-02,  1.2108e-02, -1.7581e-04,
        -2.9132e-02, -1.1230e-02, -1.5597e-03, -2.2919e-02, -1.3548e-02,
        -1.4232e-02,  1.2647e-02, -2.6115e-02,  1.6248e-02,  2.5163e-02,
         1.7973e-02,  2.3394e-02,  1.8191e-02, -1.8536e-02, -2.4553e-02,
         1.6450e-02,  9.9139e-03,  1.3807e-02, -2.0307e-02, -1.4508e-02,
        -6.3380e-03, -5.6879e-03, -7.5606e-03, -2.4678e-02,  7.6778e-03,
        -4.2413e-03,  2.0345e-02,  1.3531e-02, -1.0355e-02, -1.9800e-02,
        -1.0419e-02,  1.9542e-02, -8.2648e-03,  6.0839e-03, -2.1184e-02,
        -2.4969e-02, -2.5090e-02, -4.4691e-04,  1.4222e-04, -1.5034e-02,
        -2.5401e-02, -2.5525e-02, -1.3509e-02, -2.7451e-02, -1.2892e-03,
        -7.1181e-03,  5.7412e-03,  2.4554e-02, -2.5995e-02,  2.0607e-02,
        -8.4275e-03,  2.0365e-03, -8.2888e-03, -4.0292e-04, -2.5344e-02,
        -1.8485e-02,  1.7210e-02, -2.5891e-02,  2.0930e-02,  2.2241e-02,
         1.6016e-02,  1.4943e-02,  9.7201e-03,  5.7782e-03, -3.0357e-03,
        -1.7487e-02,  1.1248e-02, -2.5833e-02,  2.4525e-02, -2.1759e-02,
         2.3743e-02,  1.5683e-02, -1.5439e-02,  7.5573e-03,  2.0685e-02,
         1.1996e-02,  2.6734e-02,  1.4371e-02, -1.6786e-02,  4.3494e-03,
         3.7340e-03, -1.5580e-03, -5.7110e-03, -1.7264e-02,  2.2270e-02,
         8.2505e-03, -3.9274e-03,  2.0306e-03, -2.6897e-02, -7.9348e-03,
        -1.3378e-02, -1.0013e-02, -9.4395e-03, -1.5954e-02,  3.4817e-03,
         7.5906e-03, -2.3029e-02, -1.6040e-02,  8.2188e-03,  2.1579e-03,
         2.2144e-02,  7.7667e-04, -8.8603e-03, -1.2455e-03, -2.5083e-02,
        -4.5405e-03,  6.5354e-03,  1.3946e-02, -1.7964e-02, -2.8502e-03,
        -2.3017e-02, -2.2156e-02,  1.0818e-02, -2.3291e-02, -2.1803e-03,
         1.4799e-02, -1.9111e-02,  4.0326e-03, -1.9068e-02,  5.0633e-03,
         6.6018e-03, -8.3354e-03, -1.5120e-02,  2.7197e-03, -6.9961e-03,
        -2.1206e-03, -6.1386e-03])), ('linear_layer_stack.2.weight', tensor([[-0.0400, -0.0143, -0.0286,  ..., -0.1019, -0.0272, -0.0508],
        [-0.0204, -0.0052, -0.0330,  ..., -0.0229, -0.0016, -0.0229],
        [-0.0685,  0.0407,  0.0264,  ..., -0.0800,  0.0084, -0.0333],
        ...,
        [ 0.0026,  0.0219,  0.0266,  ...,  0.0076,  0.0141, -0.0378],
        [-0.0267, -0.0341, -0.0373,  ..., -0.0169,  0.0136,  0.0102],
        [-0.0455, -0.0053,  0.0010,  ..., -0.0557,  0.0283, -0.0410]])), ('linear_layer_stack.2.bias', tensor([ 0.0496,  0.0436, -0.0479,  0.0772,  0.0387,  0.0415,  0.0774, -0.0295,
         0.0238, -0.0476,  0.0227, -0.0160,  0.0074, -0.0170,  0.0900, -0.0567,
         0.1532, -0.0486, -0.0089,  0.0617,  0.0922,  0.0612, -0.0355,  0.0410,
        -0.0370,  0.0977,  0.1784,  0.1187,  0.0584,  0.0286, -0.0211,  0.0468,
         0.1227, -0.0132,  0.1213,  0.1111,  0.0431,  0.0315,  0.0957,  0.1042,
        -0.0859,  0.0260,  0.0215,  0.0775,  0.0764, -0.0058, -0.0022,  0.1075,
         0.1044,  0.0990, -0.0925, -0.0268,  0.0180,  0.1239, -0.0003,  0.0314,
         0.1066, -0.0936, -0.0700, -0.0009,  0.0248, -0.0909,  0.0286, -0.0025])), ('linear_layer_stack.4.weight', tensor([[-2.3999e-01, -3.5366e-01,  2.4567e-01, -3.4164e-01, -2.0167e-01,
          1.7481e-01,  6.8237e-02,  2.7072e-01,  8.0803e-02, -5.7576e-02,
         -8.8179e-02,  7.1352e-02, -1.4463e-01,  1.0596e-01,  1.1570e-01,
          2.4871e-01, -5.1184e-01,  1.2150e-01,  2.3942e-01, -2.5693e-02,
          2.7066e-01, -2.0429e-01, -2.7201e-01,  2.2139e-01, -1.5837e-01,
          5.0587e-02,  2.1764e-01,  2.5400e-01,  8.5060e-02,  2.1893e-01,
         -4.0745e-01,  2.2083e-01, -2.7497e-01, -2.7438e-01,  2.1672e-01,
          2.1844e-01, -2.3902e-01, -2.7577e-01,  1.9127e-01,  1.9489e-01,
          3.9070e-02,  3.1791e-01, -1.3801e-01, -4.6114e-01,  1.9982e-01,
          2.4856e-01, -3.5477e-01, -2.8655e-01,  1.9562e-01,  1.1122e-01,
         -3.1651e-01,  4.9154e-02, -4.6617e-01, -3.6802e-01,  1.4036e-01,
          1.9354e-01,  7.5866e-02, -2.2167e-01, -2.1901e-01,  2.9687e-01,
          6.7717e-03, -2.6302e-01,  1.5032e-01, -1.9253e-01],
        [ 1.8235e-01,  1.2969e-01, -1.6152e-01, -1.0245e-01, -1.0537e-02,
          3.0946e-02, -9.3360e-02,  1.0920e-01,  3.0932e-02,  8.4296e-02,
         -8.6107e-02,  4.8213e-02,  1.0949e-01,  8.6722e-03, -3.6543e-02,
          1.0067e-01,  2.1487e-01,  2.5052e-02,  9.4658e-02, -1.8003e-02,
         -6.1853e-02,  2.1477e-01, -6.0838e-02, -1.0361e-01,  3.2194e-02,
         -3.5995e-02, -2.1160e-01, -1.5024e-01, -1.2258e-01, -1.6200e-01,
          4.2999e-02, -1.4147e-01,  1.0239e-01, -2.5795e-02, -1.7782e-01,
         -1.0568e-01, -8.7622e-02,  1.3990e-01, -1.3852e-01, -4.6520e-02,
          1.0868e-01, -4.6126e-02,  5.9761e-02,  3.4530e-01, -7.7084e-03,
         -8.0778e-02, -1.0039e-01,  1.5216e-01,  7.4665e-02,  6.3615e-03,
          5.1813e-02,  1.0373e-01, -1.0580e-01,  1.1147e-01,  9.2615e-02,
         -1.8015e-01,  3.5300e-02,  9.5760e-02,  7.9911e-02,  1.1921e-01,
         -9.3337e-02,  4.0383e-02, -2.8259e-02,  1.8196e-01],
        [ 1.0823e-01,  3.9272e-02, -3.3599e-02,  1.8295e-01, -3.3138e-01,
         -4.4580e-01, -4.9031e-01, -5.7669e-03, -2.1574e-01,  1.1894e-01,
         -2.8287e-01,  2.2466e-01, -2.3703e-01, -5.6749e-02,  1.5453e-01,
         -1.9183e-01,  1.3616e-01, -1.6215e-01,  3.1279e-01,  1.9522e-01,
         -5.5650e-01,  1.0712e-01,  1.8895e-01,  1.0462e-01,  1.9892e-01,
         -2.6430e-01, -3.8812e-01, -4.4842e-01, -3.4464e-01, -5.8493e-01,
         -2.1466e-01, -3.0060e-01,  4.8232e-02,  1.7389e-01, -3.2557e-01,
         -3.8169e-01, -7.3502e-02,  1.2305e-01,  1.7791e-01,  2.1261e-01,
         -1.9210e-01, -1.2462e-01,  2.4014e-01,  1.3135e-01,  1.8151e-01,
         -1.5245e-01, -1.0517e-01, -2.4114e-03, -3.4498e-01, -4.5231e-01,
         -8.9473e-02,  6.4297e-02, -7.4779e-02, -1.7519e-01, -1.4622e-01,
          2.2453e-01,  2.0859e-01,  2.5796e-01,  2.5988e-01,  8.0607e-02,
          1.2800e-01, -1.5930e-01,  2.2706e-01,  2.4910e-02],
        [-1.5050e-01,  6.2570e-02,  3.4642e-02,  2.6447e-02,  2.9318e-02,
          7.8295e-02,  1.0287e-02, -9.2194e-02, -2.0261e-03, -5.3043e-02,
         -6.4749e-02, -1.5477e-01,  7.9053e-02,  1.1255e-01, -1.8041e-01,
          1.8510e-01, -1.3255e-04,  1.0520e-01, -9.8681e-02, -2.5967e-01,
          2.2010e-01,  4.1697e-02, -1.3316e-01,  9.4124e-02,  6.5845e-02,
          1.6711e-01,  2.0433e-01,  2.4073e-01,  1.5679e-01,  2.1340e-01,
         -3.2951e-03,  2.7701e-01,  4.3893e-02, -1.4038e-01,  1.3154e-01,
          2.8851e-01, -7.1208e-02, -1.1410e-01,  2.3849e-01,  4.7342e-02,
         -5.3330e-03, -5.4381e-02,  5.4986e-02, -1.8497e-01, -9.5875e-02,
         -8.2312e-02, -1.2298e-01,  2.5901e-02, -2.8240e-02, -4.3464e-02,
         -1.3600e-01,  1.3750e-01, -2.1643e-02,  1.3690e-01, -1.2471e-01,
          2.2538e-01, -2.5659e-02,  2.3096e-02, -1.6325e-01,  6.0860e-02,
         -9.7146e-03,  1.3452e-01,  7.3040e-02, -1.0966e-01],
        [ 6.8292e-02,  2.6392e-02, -1.1983e-01,  8.4001e-02, -1.1033e-01,
         -3.0217e-02,  1.0272e-01,  3.6719e-02, -2.2285e-02, -1.1193e-01,
          9.8902e-02,  1.1412e-01, -1.2297e-01,  6.9189e-02,  8.5342e-02,
          5.7224e-02, -9.4529e-02,  2.6911e-02, -4.8019e-02, -7.4766e-02,
         -8.3453e-02,  1.0723e-01, -7.5541e-02, -5.8089e-02, -1.1253e-01,
         -9.5410e-03, -7.2494e-02,  7.1173e-02, -1.2426e-01,  6.1374e-02,
         -7.3815e-02, -1.4037e-02, -3.1858e-02,  3.1483e-02,  1.2026e-01,
          2.9232e-02,  1.5950e-02,  5.1406e-02, -1.0399e-01,  3.2684e-02,
         -5.6037e-02, -5.0730e-02,  3.3081e-02,  4.4064e-02,  5.8388e-02,
          9.2186e-02,  7.5474e-02, -1.1975e-01, -1.0177e-01,  4.5813e-02,
         -6.4725e-02,  2.9091e-02,  6.9250e-02,  4.8992e-02,  2.6627e-02,
         -1.1193e-01, -2.1423e-03, -1.1832e-01, -5.7128e-02,  2.7581e-02,
          6.7233e-02,  3.1404e-02, -6.8358e-04, -8.3032e-02],
        [-6.6773e-02, -6.8713e-02,  3.6105e-02,  2.0357e-03,  1.0803e-01,
          1.2901e-01,  1.8458e-03, -3.2092e-02,  1.0932e-01,  1.6771e-02,
          1.0731e-01,  1.1421e-01, -1.1275e-01,  5.2697e-02,  1.8735e-01,
         -7.3513e-02,  1.1485e-01,  1.1831e-01,  7.8478e-02,  1.1174e-01,
          1.6009e-01, -1.1146e-01, -1.6835e-01,  2.0073e-01, -1.1207e-01,
          5.0491e-04,  7.2427e-02,  1.1286e-01, -1.1279e-01,  9.1834e-02,
         -8.2164e-02,  5.9208e-02,  1.2971e-02,  3.9093e-02, -1.0871e-01,
         -6.4108e-04,  2.6276e-02, -7.8267e-02,  1.4267e-01,  7.9059e-02,
          9.2206e-02, -5.0051e-02, -6.3597e-02,  9.8896e-02, -3.4079e-02,
          1.8420e-01, -1.6549e-01, -1.3864e-01, -6.0794e-03,  1.0210e-01,
         -1.4325e-01,  1.4726e-01, -2.4480e-02, -1.5404e-01,  1.5286e-01,
          7.5670e-02,  3.0209e-03, -1.3750e-01, -9.5472e-02, -1.3904e-02,
         -1.6633e-02,  1.0580e-01,  1.7160e-01, -1.2656e-01],
        [-1.5222e-01, -1.6441e-01,  1.3950e-01, -1.1433e-01, -1.8235e-01,
         -5.7650e-02,  1.4872e-02,  5.7131e-02, -2.7995e-02, -1.0188e-01,
          8.8830e-02, -1.6289e-01, -2.7250e-02, -3.0470e-02, -3.8624e-02,
          5.5610e-02, -1.0021e-01,  8.1667e-02,  1.5347e-01,  7.7056e-02,
          1.1445e-02, -1.7770e-01, -1.5982e-01,  1.8146e-01,  7.5843e-03,
          1.1622e-01, -1.6751e-01,  1.4790e-01,  7.6010e-02,  1.6810e-01,
          3.6052e-02,  5.1392e-02, -2.9939e-01, -1.7208e-01,  1.1400e-01,
          8.1451e-02, -2.2930e-01, -2.4595e-01,  1.2651e-02,  1.1337e-01,
          1.9619e-01, -3.2493e-02, -2.8892e-01, -2.4052e-02, -2.5799e-02,
          8.7030e-02, -2.2842e-01, -1.3169e-01,  1.8349e-01, -1.5396e-02,
         -9.5951e-02,  2.2723e-01, -1.6211e-01, -9.2164e-02,  5.1854e-02,
          4.3774e-02,  1.9271e-01, -2.3068e-01, -7.1880e-02,  3.2199e-02,
          1.2481e-02,  1.7557e-01,  9.2129e-02, -2.8916e-01],
        [-1.7286e-02, -5.8519e-02,  8.5553e-02, -1.0465e-01,  7.3657e-02,
          1.3578e-01, -4.5615e-03,  7.5692e-02, -9.9014e-02,  2.8254e-02,
         -9.9027e-02,  8.6122e-02, -9.6847e-02, -6.1163e-02,  5.1444e-02,
         -8.1153e-02,  1.2253e-01, -1.1242e-01,  5.5538e-02,  3.8917e-02,
         -1.9937e-02, -1.6527e-02, -7.6743e-02,  8.9884e-02, -9.2641e-02,
          5.9221e-02,  8.2285e-02,  2.9170e-02, -7.6188e-02, -1.2469e-01,
          1.1417e-01, -7.1159e-03,  2.4269e-02, -7.8558e-02,  1.9534e-02,
          6.7719e-02, -9.0236e-02,  7.3041e-02, -6.6677e-02,  1.3764e-01,
         -2.8140e-02,  1.1928e-01, -4.5402e-02,  1.2688e-02,  5.5311e-02,
         -1.1417e-01, -7.4207e-02, -8.4697e-02, -3.2623e-02,  8.7488e-02,
         -3.2211e-02,  2.1990e-02,  6.2628e-02, -6.7189e-02, -4.0889e-02,
         -9.8206e-02, -1.2550e-01, -4.6604e-02,  5.9301e-02, -4.9871e-02,
         -9.4069e-02,  1.0815e-01, -4.2603e-02,  1.2517e-01]])), ('linear_layer_stack.4.bias', tensor([ 0.2971, -0.0732,  0.4050, -0.1871, -0.1597,  0.0247,  0.2824, -0.0276])), ('linear_layer_stack.6.weight', tensor([[ 0.8065,  0.5156, -0.8892, -0.6834, -0.0919,  0.2458,  0.4427,  0.1355]])), ('linear_layer_stack.6.bias', tensor([0.6888])), ('linear_layer_stack.8.weight', tensor([[0.6503]])), ('linear_layer_stack.8.bias', tensor([0.0079]))]) 
 